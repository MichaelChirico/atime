<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Comparing data.table with tidyverse}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

In this vignette we compare computational requirements (time and
memory) of common operations using `data.table` and tidyverse functions.

## Setup

```{r}
library(data.table)
is.toby <- system("whoami", intern=TRUE) %in% c("tdhock", "nau\\th798")
seconds.limit <- if(is.toby)1 else 0.01
aplot <- function(atime.list, my.title, xmax, xlab){
  best.list <- atime::references_best(atime.list)
  both.dt <- best.list$meas
  if(require(ggplot2)){
    hline.df <- with(atime.list, data.frame(seconds.limit, unit="seconds"))
    gg <- ggplot()+
      ggtitle(my.title)+
      theme_bw()+
      facet_grid(unit ~ ., scales="free")+
      geom_hline(aes(
        yintercept=seconds.limit),
        color="grey",
        data=hline.df)+
      geom_line(aes(
        N, empirical, color=expr.name),
        data=best.list$meas)+
      geom_ribbon(aes(
        N, ymin=min, ymax=max, fill=expr.name),
        data=best.list$meas[unit=="seconds"],
        alpha=0.5)+
      scale_x_log10(xlab)+
      scale_y_log10("median line, min/max band")
    if(require(directlabels)){
      gg+
        directlabels::geom_dl(aes(
          N, empirical, color=expr.name, label=expr.name),
          method="right.polygons",
          data=best.list$meas)+
        theme(legend.position="none")+
        coord_cartesian(xlim=c(NA,xmax))
    }else{
      gg
    }
  }
}
```


## Writing CSV

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.cols <- 10
    set.seed(1)
    mat <- matrix(rnorm(N*N.cols), N, N.cols)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_real_cols_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 random normal real columns", 1e7, "Number of rows")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.cols <- 10
    mat <- matrix("data", N, N.cols)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_chr_cols_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 character columns", 1e7, "Number of rows")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.rows <- 10
    set.seed(1)
    mat <- matrix(rnorm(N.rows*N), N.rows, N)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_real_rows_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 random normal real rows", 1e7, "Number of columns")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.rows <- 10
    mat <- matrix("data", N.rows, N)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_chr_rows_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 character rows", 1e7, "Number of columns")
```

The comparisons above show significant advantages for `data.table` for
writing CSV data with a large number of columns:

* asymptotically less memory 
* constant factors less time

## Reading CSV

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_real_cols_fwrite*")),
  N.rows="[0-9]+", as.integer,
  ".csv")[order(N.rows)]
atime.list <- atime::atime(
  N=csv.dt$N.rows,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_real_cols_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 real columns", 1e7, "Number of rows")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_chr_cols_fwrite*")),
  N.rows="[0-9]+", as.integer,
  ".csv")[order(N.rows)]
atime.list <- atime::atime(
  N=csv.dt$N.rows,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_chr_cols_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 character columns", 1e7, "Number of rows")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_real_rows_fwrite*")),
  N.cols="[0-9]+", as.integer,
  ".csv")[order(N.cols)]
atime.list <- atime::atime(
  N=csv.dt$N.cols,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_real_rows_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 real rows", 1e7, "Number of columns")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_chr_rows_fwrite*")),
  N.cols="[0-9]+", as.integer,
  ".csv")[order(N.cols)]
atime.list <- atime::atime(
  N=csv.dt$N.cols,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_chr_rows_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 character rows", 1e7, "Number of columns")
```

From the comparisons above it can be seen that fread uses much less
time and memory than the alternatives.

## Summarize by group

The next problem is motivated by a common operation in machine
learning code: computing the mean/SD over cross-validation folds.

```{r}
atime.list <- atime::atime(
  N=as.integer(10^seq(0, 7, by=0.5)),
  setup={
    n.folds <- 10
    loss.dt <- data.table(
      name="loss", 
      fold=rep(1:n.folds, each=2*N),
      loss=rnorm(2*N*n.folds),
      set=rep(c("subtrain","validation"),each=N),
      epoch=1:N,
      key=c("set","epoch","fold"))
  },
  results=FALSE,
  seconds.limit=seconds.limit,
  "base::by"={
    base::by(
      loss.dt$loss, 
      list(loss.dt$set, loss.dt$epoch), 
      function(values)c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values)))
  },
  "base::tapply"={
    base::tapply(
      loss.dt$loss, 
      list(loss.dt$set, loss.dt$epoch), 
      function(values)c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values)))
  }, 
  "stats::aggregate"={
    res <- stats::aggregate(
      loss ~ set + epoch, 
      loss.dt, 
      function(values)list(c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values))))
    data.frame(
      subset(res, select=-loss), 
      do.call(rbind, res$loss))
  },
  "data.table::`[.data.table`"={
    loss.dt[, .(
      loss_length=.N,
      loss_mean=mean(loss),
      loss_sd=sd(loss)
    ), by=.(set, epoch)]
  }, 
  "data.table::dcast"={
    dcast(
      loss.dt,
      set + epoch ~ .,
      list(length, mean, sd),
      value.var="loss")
  }, 
  "dplyr::summarise"={
    loss.dt |> 
      dplyr::group_by(set, epoch) |> 
      dplyr::summarise(
        loss_length=length(loss),
        loss_mean=mean(loss), 
        loss_sd=sd(loss))
  }, 
  "tidyr::pivot_wider"={
    loss.dt |> 
      tidyr::pivot_wider(
        id_cols = c(set,epoch), 
        values_from=loss, 
        names_from=name, 
        values_fn=function(values)list(c(
          loss_length=length(values),
          loss_mean=mean(values), 
          loss_sd=sd(values)))) |> 
      tidyr::unnest_wider(loss)
  })
aplot(atime.list, "Length, Mean, SD over 10 folds for each epoch and set", 1e7, "Number of epochs")
```

The comparison above shows that using `[.data.table` is by far the
fastest way to compute the Mean and SD over folds.
