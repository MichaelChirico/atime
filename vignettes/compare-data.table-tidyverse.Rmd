<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Comparing data.table with tidyverse}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

In this vignette we compare computational requirements (time and
memory) of common operations using `data.table` and tidyverse functions.

## Setup

```{r}
library(data.table)
as.integer(Sys.getenv("SLURM_JOB_CPUS_PER_NODE", "1"))
max.threads <- parallel::detectCores()
is.toby <- system("whoami", intern=TRUE) %in% c("tdhock", "nau\\th798")
##seconds.limit <- if(is.toby)1 else 0.01
seconds.limit <- 0.01
aplot <- function(atime.list, my.title, xmax, xlab){
  best.list <- atime::references_best(atime.list)
  both.dt <- best.list$meas
  if(require(ggplot2)){
    hline.df <- with(atime.list, data.frame(seconds.limit, unit="seconds"))
    gg <- ggplot()+
      ggtitle(my.title)+
      theme_bw()+
      facet_grid(unit ~ ., scales="free")+
      geom_hline(aes(
        yintercept=seconds.limit),
        color="grey",
        data=hline.df)+
      geom_line(aes(
        N, empirical, color=expr.name),
        data=best.list$meas)+
      geom_ribbon(aes(
        N, ymin=min, ymax=max, fill=expr.name),
        data=best.list$meas[unit=="seconds"],
        alpha=0.5)+
      scale_x_log10(xlab)+
      scale_y_log10("median line, min/max band")
    if(require(directlabels)){
      gg+
        directlabels::geom_dl(aes(
          N, empirical, color=expr.name, label=expr.name),
          method="right.polygons",
          data=best.list$meas)+
        theme(legend.position="none")+
        coord_cartesian(xlim=c(NA,xmax))
    }else{
      gg
    }
  }
}
```


## Writing CSV

```{r}
threads.vec <- unique(as.integer(c(1, max.threads/2, max.threads)))
expr.list <- atime::atime_grid(
  list(THREADS=threads.vec),
  "data.table::fwrite"={
    data.table::setDTthreads(THREADS)
    data.table::fwrite(dt, name.list$fwrite, showProgress = FALSE)
  }, 
  "readr::write_csv"={
    readr::write_csv(
      dt, name.list$write_csv, progress = FALSE, num_threads = THREADS)
  })
atime_write <- function(make.mat.fun, fmt){
  atime::atime(
    N=10^seq(1, 7),
    setup={
      mat <- make.mat.fun(N)
      name.list <- list()
      for(fun in c("fwrite", "write_csv", "write.csv")){
        name.list[[fun]] <- file.path(
          tempdir(), sprintf(fmt, fun, N))
      }
      dt <- data.table(mat)
    },
    seconds.limit = seconds.limit,
    results=FALSE,
    expr.list=expr.list,
    "utils::write.csv"=utils::write.csv(dt, name.list$write.csv))
}
atime.list <- atime_write(function(N, N.cols=10){
  set.seed(1)
  matrix(rnorm(N*N.cols), N, N.cols)
}, "10_real_cols_%s_%d.csv")
c("#543005",#dark brown
  "#8C510A", "#BF812D", "#DFC27D", "#F6E8C3",
  "#F5F5F5",  #white
  "#C7EAE5", "#80CDC1", "#35978F", "#01665E",
  "#003C30")#dark teal
expr.colors <- c(
  "#67001F",#dark red
  "#B2182B", "#D6604D", "#F4A582", "#FDDBC7",
  "#FFFFFF", #white
  "#E0E0E0", "#BABABA", "#878787", "#4D4D4D",
  "#1A1A1A",
  "utils::write.csv"="deepskyblue")#almost black
names(expr.colors)[c(5,3,1)] <- paste0(
  "data.table::fwrite\nTHREADS=",threads.vec)
names(expr.colors)[c(7,9,11)] <- paste0(
  "readr::write_csv\nTHREADS=",threads.vec)
some.colors <- expr.colors[names(expr.colors)!=""]
aplot(atime.list, "Write CSV with 10 random normal real columns", 1e7, "Number of rows")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.cols <- 10
    mat <- matrix("data", N, N.cols)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_chr_cols_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 character columns", 1e7, "Number of rows")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.rows <- 10
    set.seed(1)
    mat <- matrix(rnorm(N.rows*N), N.rows, N)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_real_rows_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 random normal real rows", 1e7, "Number of columns")
```

```{r}
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    N.rows <- 10
    mat <- matrix("data", N.rows, N)
    name.list <- list()
    for(fun in c("fwrite", "write_csv", "write.csv")){
      name.list[[fun]] <- file.path(
        tempdir(), sprintf("10_chr_rows_%s_%d.csv", fun, N))
    }
    dt <- data.table(mat)
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::write.csv"=utils::write.csv(dt, name.list$write.csv),
  "data.table::fwrite"=data.table::fwrite(dt, name.list$fwrite),
  "readr::write_csv"=readr::write_csv(dt, name.list$write_csv))
aplot(atime.list, "Write CSV with 10 character rows", 1e7, "Number of columns")
```

The comparisons above show significant advantages for `data.table` for
writing CSV data with a large number of columns:

* asymptotically less memory 
* constant factors less time

## Reading CSV

```{r}
atime::atime_grid(
  list(THREADS=threads.vec, LAZY=c(TRUE, FALSE)),
  "readr::read_csv"={
    readr::read_csv(f.csv, num_threads = THREADS, lazy = LAZY)
  })
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_real_cols_fwrite*")),
  N.rows="[0-9]+", as.integer,
  ".csv")[order(N.rows)]
atime.list <- atime::atime(
  N=csv.dt$N.rows,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_real_cols_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 real columns", 1e7, "Number of rows")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_chr_cols_fwrite*")),
  N.rows="[0-9]+", as.integer,
  ".csv")[order(N.rows)]
atime.list <- atime::atime(
  N=csv.dt$N.rows,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_chr_cols_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 character columns", 1e7, "Number of rows")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_real_rows_fwrite*")),
  N.cols="[0-9]+", as.integer,
  ".csv")[order(N.cols)]
atime.list <- atime::atime(
  N=csv.dt$N.cols,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_real_rows_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 real rows", 1e7, "Number of columns")
```

```{r}
csv.dt <- nc::capture_first_vec(
  Sys.glob(file.path(tempdir(), "10_chr_rows_fwrite*")),
  N.cols="[0-9]+", as.integer,
  ".csv")[order(N.cols)]
atime.list <- atime::atime(
  N=csv.dt$N.cols,
  setup={
    f.csv <- file.path(tempdir(), sprintf("10_chr_rows_fwrite_%d.csv", N))
  },
  seconds.limit = seconds.limit,
  results=FALSE,
  "utils::read.csv"=utils::read.csv(f.csv),
  "data.table::fread"=data.table::fread(f.csv),
  "readr::read_csv"=readr::read_csv(f.csv))
aplot(atime.list, "Read CSV with 10 character rows", 1e7, "Number of columns")
```

From the comparisons above it can be seen that fread uses much less
time and memory than the alternatives.

## Summarize by group

The next problem is motivated by a common operation in machine
learning code: computing the mean/SD over cross-validation folds.

```{r}
atime.list <- atime::atime(
  N=as.integer(10^seq(0, 7, by=0.5)),
  setup={
    n.folds <- 10
    loss.dt <- data.table(
      name="loss", 
      fold=rep(1:n.folds, each=2*N),
      loss=rnorm(2*N*n.folds),
      set=rep(c("subtrain","validation"),each=N),
      epoch=1:N,
      key=c("set","epoch","fold"))
  },
  results=FALSE,
  seconds.limit=seconds.limit,
  "base::by"={
    base::by(
      loss.dt$loss, 
      list(loss.dt$set, loss.dt$epoch), 
      function(values)c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values)))
  },
  "base::tapply"={
    base::tapply(
      loss.dt$loss, 
      list(loss.dt$set, loss.dt$epoch), 
      function(values)c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values)))
  }, 
  "stats::aggregate"={
    res <- stats::aggregate(
      loss ~ set + epoch, 
      loss.dt, 
      function(values)list(c(
        loss_length=length(values),
        loss_mean=mean(values), 
        loss_sd=sd(values))))
    data.frame(
      subset(res, select=-loss), 
      do.call(rbind, res$loss))
  },
  "data.table::`[.data.table`"={
    loss.dt[, .(
      loss_length=.N,
      loss_mean=mean(loss),
      loss_sd=sd(loss)
    ), by=.(set, epoch)]
  }, 
  "data.table::dcast"={
    dcast(
      loss.dt,
      set + epoch ~ .,
      list(length, mean, sd),
      value.var="loss")
  }, 
  "dplyr::summarise"={
    loss.dt |> 
      dplyr::group_by(set, epoch) |> 
      dplyr::summarise(
        loss_length=length(loss),
        loss_mean=mean(loss), 
        loss_sd=sd(loss))
  }, 
  "tidyr::pivot_wider"={
    loss.dt |> 
      tidyr::pivot_wider(
        id_cols = c(set,epoch), 
        values_from=loss, 
        names_from=name, 
        values_fn=function(values)list(c(
          loss_length=length(values),
          loss_mean=mean(values), 
          loss_sd=sd(values)))) |> 
      tidyr::unnest_wider(loss)
  })
aplot(atime.list, "Length, Mean, SD over 10 folds for each epoch and set", 1e7, "Number of epochs")
```

The comparison above shows that using `[.data.table` is by far the
fastest way to compute the Mean and SD over folds.

## Join / merge

```{r}
i <- 1:100
DT <- CJ(letter=LETTERS, i)[, x := rnorm(.N)]
setkey(DT, letter, i)
DF <- data.frame(DT)
rownames(DF) <- with(DF, paste0(letter, i))
atime.list <- atime::atime(
  N=10^seq(1, 7),
  setup={
    select.dt <- data.table(
      letter=sample(LETTERS, N, replace=TRUE),
      i=sample(i, N, replace=TRUE),
      y=rnorm(N))
    setkey(select.dt, letter, i)
    select.df <- data.frame(select.dt)
  },
  "data.table::`[.data.table`"=DT[select.dt, x+y],
  "data.table::merge"=data.table::merge.data.table(DT,select.dt)[, x+y],
  "base::merge.data.frame"=with(base::merge.data.frame(DF, select.df, by=c('letter','i')), x+y),
  "[+paste0"=with(select.df, DF[paste0(letter,i),"x"]+y),
  "dplyr::inner_join"=with(dplyr::inner_join(DT, select.dt, by=c('letter','i')), x+y))
aplot(atime.list, "Join and sum", 1e7, "Size of output vector")
```

## Join and summarize

```{r}
i <- 1:100
DT <- CJ(letter=LETTERS, i)[, x := rnorm(.N)]
setkey(DT, letter, i)
DF <- data.frame(DT)
rownames(DF) <- with(DF, paste0(letter, i))
atime.list <- atime::atime(
  N=as.integer(10^seq(0, 7, by=0.5)),
  setup={
    select.dt <- data.table(
      letter=sample(LETTERS, N, replace=TRUE),
      i=sample(i, N, replace=TRUE),
      y=rnorm(N))
    setkey(select.dt, letter, i)
    select.df <- data.frame(select.dt)
  },
  seconds.limit=seconds.limit,
  "data.table::`[.data.table`"={
    select.dt[DT, .(rows=.N, diff=mean(y)-x), by=.EACHI, nomatch=0L]
  },
  "base::by"={
    do.call(rbind, base::by(
      select.df, 
      with(select.df, paste0(letter, i)), 
      function(sdf){
        srow <- sdf[1,]
        data.frame(
          srow[,c("letter","i")],
          rows=nrow(sdf), 
          diff=mean(sdf$y)-DF[with(srow,paste0(letter,i)),"x"])
      }))
  }, 
  "dplyr::inner_join"={
    dplyr::inner_join(DT, select.dt, by=c('letter','i')) |> 
      dplyr::group_by(letter, i) |> 
      dplyr::summarise(rows=length(y), diff=mean(y)-x[1])
  })
aplot(atime.list, "Join and summarize", 1e10, "Rows in join table")
```

## Rolling join

This situation arises when you want to compute the average in a
regular grid over some irregularly spaced numbers.

```{r}
digits <- 1
grid.space <- 10^(-digits)
offset <- grid.space/2
atime.list <- atime::atime(
  N=10^seq(1:7),
  setup={
    set.seed(1)
    X <- runif(N)
    Y <- 10*X+rnorm(N)
  }, 
  "data.table::[roll=nearest"={
    irreg.dt <- data.table(X, Y, key="X")
    grid <- seq(offset, 1-offset, by=grid.space)
    reg.dt <- data.table(grid, X=grid, key="X")
    join.dt <- reg.dt[irreg.dt, roll="nearest"]
    join.dt[, .(Y.N=.N, Y.mean=mean(Y), Y.sd=sd(Y)), by=grid]
  },
  "round,data.table"={
    data.table(
      grid=round(X+offset, digits=digits)-offset,
      Y
    )[, .(
      Y.N=.N, 
      Y.mean=mean(Y), 
      Y.sd=sd(Y)
    ), by=grid]
  },
  "round,aggregate"={
    grid <- round(X+offset, digits=digits)-offset
    aggregate(Y ~ grid, FUN=function(values)c(
      N=length(values),
      mean=mean(values),
      sd=sd(values)))
  })
aplot(atime.list, "Join and summarize", 1e10, "Rows in join table")
```
